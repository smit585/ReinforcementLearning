{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Hyperparameters are properties that govern the entire training process. They include variables which determines the network structure (Number of Hidden Units etc) and the variables which determine how the network is trained (for example, Learning Rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need Hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning has created a significant impact in the field of computer vision,\n",
    "natural language processing, and speech recognition. Due to large amount of data being generated day after day, it could be used to train Deep Neural Networks and is preferred over traditional Machine Learning algorithms for higher performance and precision which has led to various successful commercial products.\n",
    "\n",
    "Even though Deep learning has been a booming field, there are certain practical aspects of it, which remains a black box. You need to understand that Applied Deep learning is a highly iterative process. THere are various hyperparameters, that are to be kept in mind before training the model.\n",
    "\n",
    "These hyperparameters play a major role in balancing the tradeoffs and fit a good generalization over the entire training dataset. Some of such tradeoff concepts are as follows\n",
    "\n",
    "## Generalization\n",
    "Suppose we have trained a classification model on 10k images with their labels. We test the model and it was able to predict labels with a mindblowing 99% accuracy.\n",
    "\n",
    "But, when we try the same model on an unseen data, the accuracy failed to perform well. This is the case of <b> Overfitting</b>\n",
    "\n",
    "Our goal while training a network is to have it generalize well over the training data which means capturing the true signal of the data, rather than memorizing the noise in the data.\n",
    "\n",
    "In statistics, it is termed as <b>\"Goodness of Fit\"</b> which refers to how well our predicted values match to the true values\n",
    "![MLMeme.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/MachineLearningGeneralization.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "Bias-Variance Tradeoff is one of the important aspects of Applied Machine Learning. It has simple and powerful implications around the model complexity and its performance.\n",
    "\n",
    "We say there's a <b>Bias</b> in a model when the algorithm is not flexible enough to generalize well from the data. Linear parametric algorithms with low complexity such as Regression and Naive Bayes tends to have a high bias.\n",
    "\n",
    "<b>Variance</b> occurs in the model when the algorithm is sensetive and highly flexible to the training data. Non-linear non-parametric algorithms with high complexity such as Decision trees, Neural Network etc tend to have a high variance.\n",
    "\n",
    "![Bias-VarianceTradeoff.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/1_kADA5Q4al9DRLoXck6_6Xw.png?raw=true)\n",
    "\n",
    "## Overfitting vs Underfitting\n",
    "\n",
    "In both the cases, the model fails to predict the unseen data.\n",
    "Algorithms <b>overfit</b> on the training data, when it memorizes the noise instead of the data. Usually, complex algorithms such as neural networks are prone to overfitting.\n",
    "\n",
    "Algorithms <b>underfit</b> on the training data when it is not able to capture the true signal from the data. Underfitted models have bad accuracy in training as well as on the test data.\n",
    "\n",
    "To reduce these tradeoffs and making sure that the Neural networks generalize well on the training data, it is imperative that the network is well architected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How should you Architect your Keras Neural network: Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the types of Hyperparameters\n",
    "\n",
    "### 1. Number of Hidden Layers and Neuron Counts\n",
    "Below information is taken from: [Keras Layers](https://keras.io/layers/core/)\n",
    "\n",
    "Layer types and when to used them:\n",
    "\n",
    "* **Activation**: Layer that simply adds an activation function, the activation function can also be specified as part of other layer type.\n",
    "* **ActivityRegularization**: Used to add L1/L2 regularization outside of a layer. Ridge(L1) and Lasso(L2) regression can also be specified as part of other layer type.\n",
    "* **Dense**: THe original neyral network type. Every neuron is connected to the next layer. The input vector is one dimensional and placing certain inputs next to each other does not have an effect.\n",
    "* **Dropout**: Dropout consists in randomly setting a fraction rate of input(0-1) at each update during training time, which helps prevent overfitting. Dropout oly occurs during training.\n",
    "* **Flatten**: Flattens the input to 1D. Does not effect the batch size.\n",
    "* **Input**: Input layer is the entry point of the network. It augments the tensor obejct with certain attributes that is used by the keras model.\n",
    "* **Lambda**: Wraps arbitrary expression as a Layer Object.\n",
    "* **Masking**: Masks (Conceals) a sequence by using mask values to skip timesteps\n",
    "* **Permute**: Permutes the dimensions of the input according to a given pattern. Useful for eg. connecting RNNs and covnets together\n",
    "* **RepeatVector**: Repeats the input N times\n",
    "* **Reshape**: Similar to Numpy reshapes\n",
    "* **SpatialDropout1D**: Similar as dropout, but drops entire 1 dimensional feature maps instead of individual elements\n",
    "* **SpatialDropout2D**: Similar as dropout, but drops entire 2 dimensional feature maps instead of individual elements\n",
    "* **SpatialDropout3D**: Similar as dropout, but drops entire 3 dimensional feature maps instead of individual elements\n",
    "\n",
    "### 2A. Activation Functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.\n",
    "\n",
    "Below information is taken from: [Activation Function Cheat Sheets](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
    "\n",
    "* **Linear**: Pass through activation function. Usually used on the output layer of a regression neural network.\n",
    "![Linear.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/linear.png?raw=true)\n",
    "* **ELU**: Exponential linear unit, tends to converge cost to zero faster and produce more accurate results. Can produce negative outputs.\n",
    "![ELU.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/elu.PNG?raw=true)\n",
    "* **SELU**: Scaled Exponential Linear unit, essentially ELU multilplied by a scaling constant.\n",
    "* **SoftPlus**: Softplus activation function $\\log(exp(x) + 1)$  [Introduced](https://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf) in 2001.\n",
    "* **tanh** Classic neural network activation function, though often replaced by relu family on modern networks.\n",
    "![tanh.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/TanH.PNG?raw=true)\n",
    "* **softsign** Softsign activation function. $x / (abs(x) + 1)$ Similar to tanh, but not widely used.\n",
    "* **RELU** - Very popular neural network activation function.  Used for hidden layers, cannot output negative values.  No trainable parameters.\n",
    "![ReLU.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Relu.PNG?raw=true)\n",
    "* **sigmoid** - Classic neural network activation.  Often used on output layer of a binary classifier.\n",
    "![Sigmoid.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/sigmoid.PNG?raw=true)\n",
    "* **hard_sigmoid** - Less computationally expensive variant of sigmoid.\n",
    "* **exponential** - Exponential (base e) activation function.\n",
    "\n",
    "\n",
    "### 2B. Advanced Activation Functions\n",
    "\n",
    "Below information is taken from: [Keras Advanced Activation Functions](https://keras.io/layers/advanced-activations/)\n",
    "\n",
    "* **LeakyReLU**: Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active, controlled by alpha hyperparameter.\n",
    "![LeakyReLU](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/LeakyRelU.PNG?raw=true)\n",
    "* **PReLU**: Parametric Rectified Linear Unit, learns the alpha hyperparameter.\n",
    "\n",
    "### 3A. Regularizaion: L1, L2\n",
    "\n",
    "* [Keras Regularization](https://keras.io/regularizers/)\n",
    "\n",
    "\n",
    "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes. \n",
    "\n",
    "In order to create less complex model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and featre are:\n",
    "\n",
    "* **Lasso Regression(L1)**: (Least Absolute Shrinkage and Selection Operator) adds \"absolute value of magnitue\" of coefficients as penalty term to the loss function\n",
    "![Lasso](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Lasso.png?raw=true)\n",
    "* **Ridge Regression(L2)**: adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
    "![Ridge](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Ridge.PNG?raw=true)\n",
    "\n",
    "<i> The key difference between the techniques is that Lasso shrinks the less important feature's coeeficient to zero thus, removing some feature altogether. Whereas, Ridge regression suppresses the less important feature but doesn't eliminate completely. So, Lasso works well for feature selection in case we have a huge number of features. </i>\n",
    "\n",
    "In our ResNet implementation, we had implemented Ridge Regression for image classifications.\n",
    "\n",
    "### 3B. Dropout\n",
    "\n",
    "* [Keras Dropout](https://keras.io/layers/core/)\n",
    "\n",
    "Dropout, as explained in the layers section, is another method to curb overfitting. A fraction of neuron information from incoming layer is dropped intentionally to make sure that the network is not over learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch Normalization\n",
    "Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "We are going to use a simple dataset : Write about dataset\n",
    "and we are going to evaluate the network with our own hyperparameters. Eventually we are going to use Bayesian Optimization to optimize those hyperparameters to minimize log loss of the network.\n",
    "\n",
    "Following we are going to encode the dataset into a feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for Job\n",
    "col = 'job'\n",
    "df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for Area\n",
    "col = 'area'\n",
    "df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Check for Missing values of income, and fill it with the column's median\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function definintion for Evaluate network, which takes in few hyperparameters and calculate the log loss of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00107: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00227: early stopping\n",
      "-0.7663188438257202\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU, PReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def evaluate_network(dropout, lr, neuronPct, neuronShrink):\n",
    "    SPLITS = 2\n",
    "    # Bootstrapping to reduce overfit\n",
    "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "    \n",
    "    # Track progress\n",
    "    mean_benchmark = []\n",
    "    epochs_needed = []\n",
    "    num = 0\n",
    "    neuronCount = int(neuronPct*5000)\n",
    "    \n",
    "    # loop through samples\n",
    "    for train, test in boot.split(x,df['product']):\n",
    "        num +=1\n",
    "        \n",
    "        # Split training and testing data\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "        x_test = x[test]\n",
    "        \n",
    "        # Construct neural network\n",
    "        model = Sequential()\n",
    "        \n",
    "        layer = 0\n",
    "        while neuronCount>25 and layer<10:\n",
    "            if layer==0:\n",
    "                model.add(Dense(neuronCount,\n",
    "                               input_dim= x.shape[1],\n",
    "                               activation= PReLU()))\n",
    "            else:\n",
    "                model.add(Dense(neuronCount, activation= PReLU()))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            neuronCount = neuronCount*neuronShrink\n",
    "            \n",
    "        # Add the output layer, and put activation as Softmax (Classification)    \n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "        # Learning rate is explicitly mentioned as it is needed to be optimized\n",
    "        model.compile(loss='categorical_crossentropy', optimizer= Adam(lr= lr))\n",
    "        \n",
    "        # Using Earlystopping callback to monitor if val_loss is decreasing, and if \n",
    "        # it starts increasing again or becomes constant, wait for 100 iterations and\n",
    "        # restore the best weights which had the least loss. This is another approach\n",
    "        # to make sure overfitting does not happen\n",
    "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100,\n",
    "                               verbose=1, mode='auto', restore_best_weights= True)\n",
    "        \n",
    "        # Train on the bootstrap sample\n",
    "        model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=[monitor],\n",
    "                 verbose= 0, epochs=1000)\n",
    "        epochs = monitor.stopped_epoch\n",
    "        epochs_needed.append(epochs)\n",
    "        \n",
    "        # Predict on the out of boot(validation)\n",
    "        pred = model.predict(x_test)\n",
    "        \n",
    "        # Measure this bootstrap's log loss\n",
    "        y_compare = np.argmax(y_test, axis=1)\n",
    "        score = metrics.log_loss(y_compare, pred)\n",
    "        mean_benchmark.append(score)\n",
    "        m1 = statistics.mean(mean_benchmark)\n",
    "        m2 = statistics.mean(epochs_needed)\n",
    "        mdev = statistics.pstdev(mean_benchmark)\n",
    "        \n",
    "    tensorflow.keras.backend.clear_session() # Clears GPU if it has any other network loaded\n",
    "    return(-m1)\n",
    "\n",
    "print(evaluate_network(\n",
    "    dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    neuronPct=0.2,\n",
    "    neuronShrink=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In evaluate network function, we have four parameters:\n",
    "* **Dropout**:What percentage of neuron data to drop from one layer to the next\n",
    "* **Learning Rate**: Parameter of the optimizer, how quicky it tends to reach the minima.\n",
    "* **Neuron Percentage**: What value of 5000 neurons to be kept in 1st layer\n",
    "* **Neuron Shrink**: By what percentage should be reduce the number of neurons in the subsequent layers\n",
    "\n",
    "When we gave constant value to them, we got a logloss of approx 76%. Now, we are going to optimize these values so that this loss can be further decreased. We are also going to note the values at which we get the least loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
