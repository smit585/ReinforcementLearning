{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Hyperparameters are properties that govern the entire training process. They include variables which determines the network structure (Number of Hidden Units etc) and the variables which determine how the network is trained (for example, Learning Rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need Hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning has created a significant impact in the field of computer vision,\n",
    "natural language processing, and speech recognition. Due to large amount of data being generated day after day, it could be used to train Deep Neural Networks and is preferred over traditional Machine Learning algorithms for higher performance and precision which has led to various successful commercial products.\n",
    "\n",
    "Even though Deep learning has been a booming field, there are certain practical aspects of it, which remains a black box. You need to understand that Applied Deep learning is a highly iterative process. THere are various hyperparameters, that are to be kept in mind before training the model.\n",
    "\n",
    "These hyperparameters play a major role in balancing the tradeoffs and fit a good generalization over the entire training dataset. Some of such tradeoff concepts are as follows\n",
    "\n",
    "## Generalization\n",
    "Suppose we have trained a classification model on 10k images with their labels. We test the model and it was able to predict labels with a mindblowing 99% accuracy.\n",
    "\n",
    "But, when we try the same model on an unseen data, the accuracy failed to perform well. This is the case of <b> Overfitting</b>\n",
    "\n",
    "Our goal while training a network is to have it generalize well over the training data which means capturing the true signal of the data, rather than memorizing the noise in the data.\n",
    "\n",
    "In statistics, it is termed as <b>\"Goodness of Fit\"</b> which refers to how well our predicted values match to the true values\n",
    "![MLMeme.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/MachineLearningGeneralization.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "Bias-Variance Tradeoff is one of the important aspects of Applied Machine Learning. It has simple and powerful implications around the model complexity and its performance.\n",
    "\n",
    "We say there's a <b>Bias</b> in a model when the algorithm is not flexible enough to generalize well from the data. Linear parametric algorithms with low complexity such as Regression and Naive Bayes tends to have a high bias.\n",
    "\n",
    "<b>Variance</b> occurs in the model when the algorithm is sensetive and highly flexible to the training data. Non-linear non-parametric algorithms with high complexity such as Decision trees, Neural Network etc tend to have a high variance.\n",
    "\n",
    "![Bias-VarianceTradeoff.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/1_kADA5Q4al9DRLoXck6_6Xw.png?raw=true)\n",
    "\n",
    "## Overfitting vs Underfitting\n",
    "\n",
    "In both the cases, the model fails to predict the unseen data.\n",
    "Algorithms <b>overfit</b> on the training data, when it memorizes the noise instead of the data. Usually, complex algorithms such as neural networks are prone to overfitting.\n",
    "\n",
    "Algorithms <b>underfit</b> on the training data when it is not able to capture the true signal from the data. Underfitted models have bad accuracy in training as well as on the test data.\n",
    "\n",
    "To reduce these tradeoffs and making sure that the Neural networks generalize well on the training data, it is imperative that the network is well architected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How should you Architect your Keras Neural network: Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the types of Hyperparameters\n",
    "\n",
    "### 1. Number of Hidden Layers and Neuron Counts\n",
    "Below information is taken from: [Keras Layers](https://keras.io/layers/core/)\n",
    "\n",
    "Layer types and when to used them:\n",
    "\n",
    "* **Activation**: Layer that simply adds an activation function, the activation function can also be specified as part of other layer type.\n",
    "* **ActivityRegularization**: Used to add L1/L2 regularization outside of a layer. Ridge(L1) and Lasso(L2) regression can also be specified as part of other layer type.\n",
    "* **Dense**: THe original neyral network type. Every neuron is connected to the next layer. The input vector is one dimensional and placing certain inputs next to each other does not have an effect.\n",
    "* **Dropout**: Dropout consists in randomly setting a fraction rate of input(0-1) at each update during training time, which helps prevent overfitting. Dropout oly occurs during training.\n",
    "* **Flatten**: Flattens the input to 1D. Does not effect the batch size.\n",
    "* **Input**: Input layer is the entry point of the network. It augments the tensor obejct with certain attributes that is used by the keras model.\n",
    "* **Lambda**: Wraps arbitrary expression as a Layer Object.\n",
    "* **Masking**: Masks (Conceals) a sequence by using mask values to skip timesteps\n",
    "* **Permute**: Permutes the dimensions of the input according to a given pattern. Useful for eg. connecting RNNs and covnets together\n",
    "* **RepeatVector**: Repeats the input N times\n",
    "* **Reshape**: Similar to Numpy reshapes\n",
    "* **SpatialDropout1D**: Similar as dropout, but drops entire 1 dimensional feature maps instead of individual elements\n",
    "* **SpatialDropout2D**: Similar as dropout, but drops entire 2 dimensional feature maps instead of individual elements\n",
    "* **SpatialDropout3D**: Similar as dropout, but drops entire 3 dimensional feature maps instead of individual elements\n",
    "\n",
    "### 2A. Activation Functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.\n",
    "\n",
    "Below information is taken from: [Activation Function Cheat Sheets](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
    "\n",
    "* **Linear**: Pass through activation function. Usually used on the output layer of a regression neural network.\n",
    "![Linear.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/linear.png?raw=true)\n",
    "* **ELU**: Exponential linear unit, tends to converge cost to zero faster and produce more accurate results. Can produce negative outputs.\n",
    "![ELU.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/elu.PNG?raw=true)\n",
    "* **SELU**: Scaled Exponential Linear unit, essentially ELU multilplied by a scaling constant.\n",
    "* **SoftPlus**: Softplus activation function $\\log(exp(x) + 1)$  [Introduced](https://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf) in 2001.\n",
    "* **tanh** Classic neural network activation function, though often replaced by relu family on modern networks.\n",
    "![tanh.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/TanH.PNG?raw=true)\n",
    "* **softsign** Softsign activation function. $x / (abs(x) + 1)$ Similar to tanh, but not widely used.\n",
    "* **RELU** - Very popular neural network activation function.  Used for hidden layers, cannot output negative values.  No trainable parameters.\n",
    "![ReLU.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Relu.PNG?raw=true)\n",
    "* **sigmoid** - Classic neural network activation.  Often used on output layer of a binary classifier.\n",
    "![Sigmoid.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/sigmoid.PNG?raw=true)\n",
    "* **hard_sigmoid** - Less computationally expensive variant of sigmoid.\n",
    "* **exponential** - Exponential (base e) activation function.\n",
    "\n",
    "\n",
    "### 2B. Advanced Activation Functions\n",
    "\n",
    "Below information is taken from: [Keras Advanced Activation Functions](https://keras.io/layers/advanced-activations/)\n",
    "\n",
    "* **LeakyReLU**: Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active, controlled by alpha hyperparameter.\n",
    "**[Put Image]**\n",
    "* **PReLU**: Parametric Rectified Linear Unit, learns the alpha hyperparameter.\n",
    "\n",
    "### 3A. Regularizaion: L1, L2\n",
    "\n",
    "* [Keras Regularization](https://keras.io/regularizers/)\n",
    "\n",
    "\n",
    "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes. \n",
    "\n",
    "In order to create less complex model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and featre are:\n",
    "\n",
    "* **Lasso Regression(L1)**: (Least Absolute Shrinkage and Selection Operator) adds \"absolute value of magnitue\" of coefficients as penalty term to the loss function\n",
    "**[Put Image]**\n",
    "* **Ridge Regression(L2)**: adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
    "**[Put Image]**\n",
    "\n",
    "<i> The key difference between the techniques is that Lasso shrinks the less important feature's coeeficient to zero thus, removing some feature altogether. Whereas, Ridge regression suppresses the less important feature but doesn't eliminate completely. So, Lasso works well for feature selection in case we have a huge number of features. </i>\n",
    "\n",
    "In our ResNet implementation, we had implemented Ridge Regression for image classifications.\n",
    "\n",
    "### 3B. Dropout\n",
    "\n",
    "* [Keras Dropout](https://keras.io/layers/core/)\n",
    "\n",
    "Dropout, as explained in the layers section, is another method to curb overfitting. A fraction of neuron information from incoming layer id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
