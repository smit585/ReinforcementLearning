{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Hyperparameters are properties that govern the entire training process. They include variables which determines the network structure (Number of Hidden Units etc) and the variables which determine how the network is trained (for example, Learning Rate). These characteristics are external to model and its values has to be set before the learning process begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need Hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning has created a significant impact in the field of computer vision,\n",
    "natural language processing, and speech recognition. Due to large amount of data being generated day after day, it could be used to train Deep Neural Networks and is preferred over traditional Machine Learning algorithms for higher performance and precision which has led to various successful commercial products.\n",
    "\n",
    "Even though Deep learning has been a booming field, there are certain practical aspects of it, which remains a black box. You need to understand that Applied Deep learning is a highly iterative process. THere are various hyperparameters, that are to be kept in mind before training the model.\n",
    "\n",
    "These hyperparameters play a major role in balancing the tradeoffs and fit a good generalization over the entire training dataset. Some of such tradeoff concepts are as follows\n",
    "\n",
    "## Generalization\n",
    "Suppose we have trained a classification model on 10k images with their labels. We test the model and it was able to predict labels with a mindblowing 99% accuracy.\n",
    "\n",
    "But, when we try the same model on an unseen data, the accuracy failed to perform well. This is the case of <b> Overfitting</b>\n",
    "\n",
    "Our goal while training a network is to have it generalize well over the training data which means capturing the true signal of the data, rather than memorizing the noise in the data.\n",
    "\n",
    "In statistics, it is termed as <b>\"Goodness of Fit\"</b> which refers to how well our predicted values match to the true values\n",
    "![MLMeme.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/MachineLearningGeneralization.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "Bias-Variance Tradeoff is one of the important aspects of Applied Machine Learning. It has simple and powerful implications around the model complexity and its performance.\n",
    "\n",
    "We say there's a <b>Bias</b> in a model when the algorithm is not flexible enough to generalize well from the data. Linear parametric algorithms with low complexity such as Regression and Naive Bayes tends to have a high bias.\n",
    "\n",
    "<b>Variance</b> occurs in the model when the algorithm is sensetive and highly flexible to the training data. Non-linear non-parametric algorithms with high complexity such as Decision trees, Neural Network etc tend to have a high variance.\n",
    "\n",
    "![Bias-VarianceTradeoff.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/1_kADA5Q4al9DRLoXck6_6Xw.png?raw=true)\n",
    "\n",
    "## Overfitting vs Underfitting\n",
    "\n",
    "In both the cases, the model fails to predict the unseen data.\n",
    "Algorithms <b>overfit</b> on the training data, when it memorizes the noise instead of the data. Usually, complex algorithms such as neural networks are prone to overfitting.\n",
    "\n",
    "Algorithms <b>underfit</b> on the training data when it is not able to capture the true signal from the data. Underfitted models have bad accuracy in training as well as on the test data.\n",
    "\n",
    "To reduce these tradeoffs and making sure that the Neural networks generalize well on the training data, it is imperative that the network is well architected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How should you Architect your Keras Neural network: Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the types of Hyperparameters\n",
    "\n",
    "### 1. Number of Hidden Layers and Neuron Counts\n",
    "Below information is taken from: [Keras Layers](https://keras.io/layers/core/)\n",
    "\n",
    "Layer types and when to used them:\n",
    "\n",
    "* **Activation**: Layer that simply adds an activation function, the activation function can also be specified as part of other layer type.\n",
    "* **ActivityRegularization**: Used to add L1/L2 regularization outside of a layer. Ridge(L1) and Lasso(L2) regression can also be specified as part of other layer type.\n",
    "* **Dense**: THe original neyral network type. Every neuron is connected to the next layer. The input vector is one dimensional and placing certain inputs next to each other does not have an effect.\n",
    "* **Dropout**: Dropout consists in randomly setting a fraction rate of input(0-1) at each update during training time, which helps prevent overfitting. Dropout oly occurs during training.\n",
    "* **Flatten**: Flattens the input to 1D. Does not effect the batch size.\n",
    "* **Input**: Input layer is the entry point of the network. It augments the tensor obejct with certain attributes that is used by the keras model.\n",
    "* **Lambda**: Wraps arbitrary expression as a Layer Object.\n",
    "* **Masking**: Masks (Conceals) a sequence by using mask values to skip timesteps\n",
    "* **Permute**: Permutes the dimensions of the input according to a given pattern. Useful for eg. connecting RNNs and covnets together\n",
    "* **RepeatVector**: Repeats the input N times\n",
    "* **Reshape**: Similar to Numpy reshapes\n",
    "* **SpatialDropout1D**: Similar as dropout, but drops entire 1 dimensional feature maps instead of individual elements\n",
    "* **SpatialDropout2D**: Similar as dropout, but drops entire 2 dimensional feature maps instead of individual elements\n",
    "* **SpatialDropout3D**: Similar as dropout, but drops entire 3 dimensional feature maps instead of individual elements\n",
    "\n",
    "### 2A. Activation Functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.\n",
    "\n",
    "Below information is taken from: [Activation Function Cheat Sheets](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
    "\n",
    "* **Linear**: Pass through activation function. Usually used on the output layer of a regression neural network.\n",
    "![Linear.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/linear.png?raw=true)\n",
    "* **ELU**: Exponential linear unit, tends to converge cost to zero faster and produce more accurate results. Can produce negative outputs.\n",
    "![ELU.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/elu.PNG?raw=true)\n",
    "* **SELU**: Scaled Exponential Linear unit, essentially ELU multilplied by a scaling constant.\n",
    "* **SoftPlus**: Softplus activation function $\\log(exp(x) + 1)$  [Introduced](https://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf) in 2001.\n",
    "* **tanh** Classic neural network activation function, though often replaced by relu family on modern networks.\n",
    "![tanh.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/TanH.PNG?raw=true)\n",
    "* **softsign** Softsign activation function. $x / (abs(x) + 1)$ Similar to tanh, but not widely used.\n",
    "* **RELU** - Very popular neural network activation function.  Used for hidden layers, cannot output negative values.  No trainable parameters.\n",
    "![ReLU.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Relu.PNG?raw=true)\n",
    "* **sigmoid** - Classic neural network activation.  Often used on output layer of a binary classifier.\n",
    "![Sigmoid.png](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/sigmoid.PNG?raw=true)\n",
    "* **hard_sigmoid** - Less computationally expensive variant of sigmoid.\n",
    "* **exponential** - Exponential (base e) activation function.\n",
    "\n",
    "\n",
    "### 2B. Advanced Activation Functions\n",
    "\n",
    "Below information is taken from: [Keras Advanced Activation Functions](https://keras.io/layers/advanced-activations/)\n",
    "\n",
    "* **LeakyReLU**: Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active, controlled by alpha hyperparameter.\n",
    "![LeakyReLU](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/LeakyRelU.PNG?raw=true)\n",
    "* **PReLU**: Parametric Rectified Linear Unit, learns the alpha hyperparameter.\n",
    "\n",
    "### 3A. Regularizaion: L1, L2\n",
    "\n",
    "* [Keras Regularization](https://keras.io/regularizers/)\n",
    "\n",
    "\n",
    "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes. \n",
    "\n",
    "In order to create less complex model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and featre are:\n",
    "\n",
    "* **Lasso Regression(L1)**: (Least Absolute Shrinkage and Selection Operator) adds \"absolute value of magnitue\" of coefficients as penalty term to the loss function\n",
    "![Lasso](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Lasso.png?raw=true)\n",
    "* **Ridge Regression(L2)**: adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
    "![Ridge](https://github.com/smit585/ReinforcementLearning/blob/BayesianOptimization/GitLab%20Images/Ridge.PNG?raw=true)\n",
    "\n",
    "<i> The key difference between the techniques is that Lasso shrinks the less important feature's coeeficient to zero thus, removing some feature altogether. Whereas, Ridge regression suppresses the less important feature but doesn't eliminate completely. So, Lasso works well for feature selection in case we have a huge number of features. </i>\n",
    "\n",
    "In our ResNet implementation, we had implemented Ridge Regression for image classifications.\n",
    "\n",
    "### 3B. Dropout\n",
    "\n",
    "* [Keras Dropout](https://keras.io/layers/core/)\n",
    "\n",
    "Dropout, as explained in the layers section, is another method to curb overfitting. A fraction of neuron information from incoming layer is dropped intentionally to make sure that the network is not over learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch Normalization\n",
    "Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "We are going to use a simple dataset : Write about dataset\n",
    "and we are going to evaluate the network with our own hyperparameters. Eventually we are going to use Bayesian Optimization to optimize those hyperparameters to minimize log loss of the network.\n",
    "\n",
    "Following we are going to encode the dataset into a feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smits\\miniconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for Job\n",
    "col = 'job'\n",
    "df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for Area\n",
    "col = 'area'\n",
    "df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Check for Missing values of income, and fill it with the column's median\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function definintion for Evaluate network, which takes in few hyperparameters and calculate the log loss of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smits\\miniconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00115: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00163: early stopping\n",
      "-0.7650102422863712\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU, PReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def evaluate_network(dropout, lr, neuronPct, neuronShrink):\n",
    "    SPLITS = 2\n",
    "    # Bootstrapping to reduce overfit\n",
    "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
    "    \n",
    "    # Track progress\n",
    "    mean_benchmark = []\n",
    "    epochs_needed = []\n",
    "    num = 0\n",
    "    neuronCount = int(neuronPct*5000)\n",
    "    \n",
    "    # loop through samples\n",
    "    for train, test in boot.split(x,df['product']):\n",
    "        num +=1\n",
    "        \n",
    "        # Split training and testing data\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "        x_test = x[test]\n",
    "        \n",
    "        # Construct neural network\n",
    "        model = Sequential()\n",
    "        \n",
    "        layer = 0\n",
    "        while neuronCount>25 and layer<10:\n",
    "            if layer==0:\n",
    "                model.add(Dense(neuronCount,\n",
    "                               input_dim= x.shape[1],\n",
    "                               activation= PReLU()))\n",
    "            else:\n",
    "                model.add(Dense(neuronCount, activation= PReLU()))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            neuronCount = neuronCount*neuronShrink\n",
    "            \n",
    "        # Add the output layer, and put activation as Softmax (Classification)    \n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "        # Learning rate is explicitly mentioned as it is needed to be optimized\n",
    "        model.compile(loss='categorical_crossentropy', optimizer= Adam(lr= lr))\n",
    "        \n",
    "        # Using Earlystopping callback to monitor if val_loss is decreasing, and if \n",
    "        # it starts increasing again or becomes constant, wait for 100 iterations and\n",
    "        # restore the best weights which had the least loss. This is another approach\n",
    "        # to make sure overfitting does not happen\n",
    "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100,\n",
    "                               verbose=1, mode='auto', restore_best_weights= True)\n",
    "        \n",
    "        # Train on the bootstrap sample\n",
    "        model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=[monitor],\n",
    "                 verbose= 0, epochs=1000)\n",
    "        epochs = monitor.stopped_epoch\n",
    "        epochs_needed.append(epochs)\n",
    "        \n",
    "        # Predict on the out of boot(validation)\n",
    "        pred = model.predict(x_test)\n",
    "        \n",
    "        # Measure this bootstrap's log loss\n",
    "        y_compare = np.argmax(y_test, axis=1)\n",
    "        score = metrics.log_loss(y_compare, pred)\n",
    "        mean_benchmark.append(score)\n",
    "        m1 = statistics.mean(mean_benchmark)\n",
    "        m2 = statistics.mean(epochs_needed)\n",
    "        mdev = statistics.pstdev(mean_benchmark)\n",
    "        \n",
    "    tensorflow.keras.backend.clear_session() # Clears GPU if it has any other network loaded\n",
    "    return(-m1)\n",
    "\n",
    "print(evaluate_network(\n",
    "    dropout=0.2,\n",
    "    lr=1e-3,\n",
    "    neuronPct=0.2,\n",
    "    neuronShrink=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In evaluate network function, we have four parameters:\n",
    "* **Dropout**:What percentage of neuron data to drop from one layer to the next\n",
    "* **Learning Rate**: Parameter of the optimizer, how quicky it tends to reach the minima.\n",
    "* **Neuron Percentage**: What value of 5000 neurons to be kept in 1st layer\n",
    "* **Neuron Shrink**: By what percentage should be reduce the number of neurons in the subsequent layers\n",
    "\n",
    "When we gave constant value to them, we got a logloss of approx 76%. Now, we are going to optimize these values so that this loss can be further decreased. We are also going to note the values at which we get the least loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "Bayesian Hyperparameter Optimization is a method of finding hyperparameters in a more efficient manner than a grid search. Because each candidate set of hyperparameters requires a retraining of the neural network, it is best to keep the number of candidates sets to minimum. Bayesian Hyperparameter Optimization achieves this by training a model to predict good candidate sets of hyperparameters\n",
    "* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n",
    "\n",
    "### How does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in c:\\users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages (from bayesian-optimization) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages (from bayesian-optimization) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages (from bayesian-optimization) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the BayesianOptimization constructor to evaluate the function we created just above. In that we are pushing the bounding values of all the hyper parameters that we do not want to cross. For example we want dropout rate to be between 0 to 50%, which is already a big number. \n",
    "\n",
    "After the Bayesian Optimizer class is created, we are using that to maximize the negative of loss, or minimize the loss. In the optimizer.maximize we are giving 2 parameters init_points and n_iter.\n",
    "\n",
    "This is analogous to n-arm bandit optimization technique, which basically has 2 parameters: Explore and Exploit.\n",
    "* Explore: Gives the number of bandit machines to look for the one that gives the highest returns\n",
    "* Exploit: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  dropout  |    lr     | neuronPct | neuron... |\n",
      "-------------------------------------------------------------------------\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00273: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00102: early stopping\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.775   \u001b[0m | \u001b[0m 0.2081  \u001b[0m | \u001b[0m 0.07203 \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 0.3093  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00215: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00219: early stopping\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.7276  \u001b[0m | \u001b[95m 0.07323 \u001b[0m | \u001b[95m 0.009234\u001b[0m | \u001b[95m 0.1944  \u001b[0m | \u001b[95m 0.3521  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1225: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00287: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00106: early stopping\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.951   \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 0.05388 \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 0.6884  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00180: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00155: early stopping\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.7324  \u001b[0m | \u001b[0m 0.102   \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.03711 \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00184: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00110: early stopping\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.7802  \u001b[0m | \u001b[0m 0.2082  \u001b[0m | \u001b[0m 0.05587 \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 0.2061  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1225: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00101: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00122: early stopping\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-9.372   \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.09683 \u001b[0m | \u001b[0m 0.3203  \u001b[0m | \u001b[0m 0.6954  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00101: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00103: early stopping\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.48    \u001b[0m | \u001b[0m 0.4373  \u001b[0m | \u001b[0m 0.08946 \u001b[0m | \u001b[0m 0.09419 \u001b[0m | \u001b[0m 0.04866 \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00131: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00110: early stopping\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.7152  \u001b[0m | \u001b[95m 0.08475 \u001b[0m | \u001b[95m 0.08781 \u001b[0m | \u001b[95m 0.1074  \u001b[0m | \u001b[95m 0.4269  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00198: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00227: early stopping\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-3.958   \u001b[0m | \u001b[0m 0.478   \u001b[0m | \u001b[0m 0.05332 \u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 0.3224  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00284: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00114: early stopping\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.006   \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 0.08346 \u001b[0m | \u001b[0m 0.02811 \u001b[0m | \u001b[0m 0.7526  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00110: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00113: early stopping\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.8355  \u001b[0m | \u001b[0m 0.1132  \u001b[0m | \u001b[0m 0.07359 \u001b[0m | \u001b[0m 0.1042  \u001b[0m | \u001b[0m 0.2925  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00101: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00105: early stopping\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-11.27   \u001b[0m | \u001b[0m 0.2208  \u001b[0m | \u001b[0m 0.04135 \u001b[0m | \u001b[0m 0.5523  \u001b[0m | \u001b[0m 0.7468  \u001b[0m |\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00147: early stopping\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00155: early stopping\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.7406  \u001b[0m | \u001b[0m 0.1337  \u001b[0m | \u001b[0m 0.04257 \u001b[0m | \u001b[0m 0.1289  \u001b[0m | \u001b[0m 0.3671  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smits\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1225: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00100: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.01058193055266179, 0.08078812396267213, 0.9652069856921562, 0.7050522969711596)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4cd733078583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                 random_state = 1)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f67f1c15f1ca>\u001b[0m in \u001b[0;36mevaluate_network\u001b[1;34m(dropout, lr, neuronPct, neuronShrink)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# Train on the bootstrap sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         model.fit(x_train, y_train, validation_data=(x_test, y_test), callbacks=[monitor],\n\u001b[1;32m---> 64\u001b[1;33m                  verbose= 0, epochs=1000)\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mepochs_needed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m                       total_epochs=1)\n\u001b[0;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 372\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    683\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Restoring model weights from the end of the best epoch.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1238\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1320\u001b[0m     \"\"\"\n\u001b[0;32m   1321\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m       raise ValueError('You called `set_weights(weights)` on layer \"' +\n\u001b[0;32m   1324\u001b[0m                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\" with a  weight list of length '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = { 'dropout': (0.0, 0.499),\n",
    "            'lr': (0.0, 0.1),\n",
    "            'neuronPct': (0.01,1),\n",
    "            'neuronShrink': (0.01, 1)}\n",
    "\n",
    "optimizer = BayesianOptimization( f= evaluate_network,\n",
    "                                pbounds = pbounds,\n",
    "                                verbose = 2,\n",
    "                                random_state = 1)\n",
    "\n",
    "optimizer.maximize(init_points = 10, n_iter=100)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
